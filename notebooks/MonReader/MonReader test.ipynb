{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Background:**\n",
    "\n",
    "Our company develops innovative Artificial Intelligence and Computer Vision solutions that revolutionize industries. Machines that can see: We pack our solutions in small yet intelligent devices that can be easily integrated to your existing data flow. Computer vision for everyone: Our devices can recognize faces, estimate age and gender, classify clothing types and colors, identify everyday objects and detect motion. Technical consultancy: We help you identify use cases of artificial intelligence and computer vision in your industry. Artificial intelligence is the technology of today, not the future.\n",
    "\n",
    "MonReader is a new mobile document digitization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor.\n",
    "\n",
    "\n",
    "\n",
    "MonReader is a new mobile document digitalization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor.\n",
    "\n",
    "\n",
    "\n",
    "##### **Data Description:**\n",
    "\n",
    "We collected page flipping video from smart phones and labelled them as flipping and not flipping.\n",
    "\n",
    "We clipped the videos as short videos and labelled them as flipping or not flipping. The extracted frames are then saved to disk in a sequential order with the following naming structure: VideoID_FrameNumber\n",
    "\n",
    "##### **Goal(s):**\n",
    "\n",
    "Predict if the page is being flipped using a single image.\n",
    "\n",
    "##### **Success Metrics:**\n",
    "\n",
    "Evaluate model performance based on F1 score, the higher the better.\n",
    "\n",
    "##### **Bonus(es):**\n",
    "\n",
    "Predict if a given sequence of images contains an action of flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load Data and Augment Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2392 images belonging to 2 classes.\n",
      "Found 597 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory setup\n",
    "train_data_dir = 'images/training'\n",
    "test_data_dir = 'images/testing'\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "#data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,  \n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#data generator for testing (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Creation and Design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture\n",
    "inputs = Input(shape=(img_width, img_height, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "#calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "#metrics\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 474ms/step - AUC: 0.6317 - accuracy: 0.5933 - f1_score: 0.5038 - loss: 0.6675 - precision_1: 0.6108 - recall_1: 0.5169 - val_AUC: 0.6944 - val_accuracy: 0.5628 - val_f1_score: 0.7023 - val_loss: 0.6806 - val_precision_1: 0.5405 - val_recall_1: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 467ms/step - AUC: 0.7894 - accuracy: 0.7278 - f1_score: 0.7465 - loss: 0.5701 - precision_1: 0.7129 - recall_1: 0.7747 - val_AUC: 0.7561 - val_accuracy: 0.4858 - val_f1_score: 0.0000e+00 - val_loss: 0.7051 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 468ms/step - AUC: 0.8522 - accuracy: 0.7825 - f1_score: 0.8148 - loss: 0.4852 - precision_1: 0.7617 - recall_1: 0.8415 - val_AUC: 0.8100 - val_accuracy: 0.4858 - val_f1_score: 0.0000e+00 - val_loss: 1.0447 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - AUC: 0.8541 - accuracy: 0.7885 - f1_score: 0.7962 - loss: 0.4689 - precision_1: 0.7807 - recall_1: 0.8179 - val_AUC: 0.8356 - val_accuracy: 0.4858 - val_f1_score: 0.0000e+00 - val_loss: 1.3257 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - AUC: 0.8753 - accuracy: 0.8133 - f1_score: 0.8318 - loss: 0.4405 - precision_1: 0.8121 - recall_1: 0.8410 - val_AUC: 0.8521 - val_accuracy: 0.4941 - val_f1_score: 0.0330 - val_loss: 0.8031 - val_precision_1: 0.6667 - val_recall_1: 0.0326\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 492ms/step - AUC: 0.8771 - accuracy: 0.8024 - f1_score: 0.7917 - loss: 0.4381 - precision_1: 0.7910 - recall_1: 0.8274 - val_AUC: 0.8701 - val_accuracy: 0.5595 - val_f1_score: 0.3245 - val_loss: 0.6739 - val_precision_1: 0.8667 - val_recall_1: 0.1694\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 493ms/step - AUC: 0.8986 - accuracy: 0.8223 - f1_score: 0.8451 - loss: 0.4023 - precision_1: 0.8171 - recall_1: 0.8581 - val_AUC: 0.8804 - val_accuracy: 0.6901 - val_f1_score: 0.6023 - val_loss: 0.5518 - val_precision_1: 0.8910 - val_recall_1: 0.4528\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 494ms/step - AUC: 0.8905 - accuracy: 0.8213 - f1_score: 0.8481 - loss: 0.4057 - precision_1: 0.8136 - recall_1: 0.8541 - val_AUC: 0.9018 - val_accuracy: 0.5611 - val_f1_score: 0.2687 - val_loss: 0.7807 - val_precision_1: 0.9091 - val_recall_1: 0.1629\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 493ms/step - AUC: 0.8942 - accuracy: 0.8224 - f1_score: 0.8286 - loss: 0.4054 - precision_1: 0.8061 - recall_1: 0.8655 - val_AUC: 0.9141 - val_accuracy: 0.8325 - val_f1_score: 0.8558 - val_loss: 0.3966 - val_precision_1: 0.7717 - val_recall_1: 0.9577\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 492ms/step - AUC: 0.8962 - accuracy: 0.8164 - f1_score: 0.8224 - loss: 0.4001 - precision_1: 0.8124 - recall_1: 0.8400 - val_AUC: 0.9178 - val_accuracy: 0.7822 - val_f1_score: 0.8202 - val_loss: 0.4449 - val_precision_1: 0.7122 - val_recall_1: 0.9674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              metrics=[f1_score, 'accuracy', precision, recall, 'AUC'])\n",
    "\n",
    "#model training\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    class_weight=class_weights)\n",
    "\n",
    "#save the model\n",
    "model.save('page_flip_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 215ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.30      0.37       290\n",
      "           1       0.52      0.70      0.59       307\n",
      "\n",
      "    accuracy                           0.51       597\n",
      "   macro avg       0.50      0.50      0.48       597\n",
      "weighted avg       0.50      0.51      0.49       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction and evaluation\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "y_true = test_generator.classes\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
